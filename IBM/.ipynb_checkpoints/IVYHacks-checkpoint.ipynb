{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Analysing Facebook (Pages & Groups) Data with IBM Watson\n",
    "\n",
    "Este es un cuaderno destinado a mostrar cómo estamos utilizando una exportación estándar de Facebook Analytics que presenta textos de publicaciones, artículos y miniaturas, junto con métricas de rendimiento estándar como me gusta, acciones e impresiones.\n",
    "\n",
    "**1st** we'll use the Natural Language Understanding and (optionally) Visual Recognition services from IBM Watson to enrich the Facebook posts, thumbnails, and articles by pulling out `Sentiment`, `Emotion`, `Entities`, `Keywords`, and `Images`. \n",
    "\n",
    "**2nd** we'll set up multiple pandas DataFrames that will contain the values, and metrics needed to find insights.\n",
    "\n",
    "**3rd** we'll use charts to visualize the features that we discovered during enrichment and show how they correlate with student impressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need these packages:\n",
    " - [Watson APIs Python SDK](https://github.com/watson-developer-cloud/python-sdk): a client library for Watson services.\n",
    " - <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\" target=\"_blank\" rel=\"noopener noreferrer\">Beautiful Soup</a>: a library to parse data from HTML for enriching the Facebook data.\n",
    " - <a href=\"https://ibm-cds-labs.github.io/pixiedust/\" target=\"_blank\" rel=\"noopener noreferrer\">PixieDust</a>: a library to visualize the data. \n",
    "\n",
    "Install the [Watson Python SDK](https://pypi.org/project/ibm-watson/) package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --user --no-warn-script-location ibm-watson==4.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the [Beautiful Soup](https://pypi.org/project/beautifulsoup4) package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --user beautifulsoup4==4.9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install [PixieDust](https://pypi.org/project/pixiedust/) Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --user --no-warn-script-location --upgrade pixiedust==1.1.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from ibm_watson import VisualRecognitionV3\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from ibm_watson.natural_language_understanding_v1 import Features, EntitiesOptions, KeywordsOptions, EmotionOptions, SentimentOptions\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from operator import itemgetter\n",
    "from os.path import join, dirname\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# Suppress some pandas warnings\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "# Suppress SSL warnings\n",
    "requests.packages.urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Service Credentials From IBM Cloud for Watson Services\n",
    "\n",
    "* Watson Natural Language Understanding service \n",
    "    * Service for [Natural Language Understanding (NLU)](https://cloud.ibm.com/catalog/services/natural-language-understanding). \n",
    "* Watson Visual Recognition service \n",
    "    * Service for [Visual Recognition](https://cloud.ibm.com/catalog/services/visual-recognition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@hidden_cell\n",
    "\n",
    "# Watson Natural Language Understanding (NLU)\n",
    "NATURAL_LANGUAGE_UNDERSTANDING_API_KEY = '7MGwCGJ9qF4zahtrQl3eN6vn2RxBqiQ5YhWZqi4Xee3E'\n",
    "NATURAL_LANGUAGE_UNDERSTANDING_URL = 'https://api.us-south.natural-language-understanding.watson.cloud.ibm.com/instances/68ba0285-8044-46d0-a3cb-6a9b334e1879'\n",
    "\n",
    "# Watson Visual Recognition\n",
    "VISUAL_RECOGNITION_API_KEY = 'pfY63VzhP5NqpR9-5AZMFxmeajkJdjxL9mBDNre_cOh-'\n",
    "VISUAL_RECOGNITION_URL = 'https://api.us-south.visual-recognition.watson.cloud.ibm.com/instances/4f4bc535-5e22-4923-816a-f4c1afb058d1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Watson clients\n",
    "\n",
    "nlu_auth = IAMAuthenticator(NATURAL_LANGUAGE_UNDERSTANDING_API_KEY)\n",
    "nlu = NaturalLanguageUnderstandingV1(version='2020-08-01',\n",
    "                                     authenticator=nlu_auth)\n",
    "nlu.set_service_url(NATURAL_LANGUAGE_UNDERSTANDING_URL)\n",
    "\n",
    "visual_recognition = False  # Making visrec optional.\n",
    "if VISUAL_RECOGNITION_API_KEY and VISUAL_RECOGNITION_URL:\n",
    "    vr_auth = IAMAuthenticator(VISUAL_RECOGNITION_API_KEY)\n",
    "    visual_recognition = VisualRecognitionV3(version='2020-08-01',\n",
    "                                             authenticator=vr_auth)\n",
    "    visual_recognition.set_service_url(VISUAL_RECOGNITION_URL)\n",
    "else:\n",
    "    print(\"Skipping Visual Recognition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2nd Loading Data\n",
    "The data you'll analyzing is a sample of a standard export of the Facebook Insights Post information from the \"ETA 16 Dolores Hidalgo C.I.N\". Engagement metrics such as clicks, impressions, and so on, are altered and do not reflect actual post performance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post ID</th>\n",
       "      <th>Permalink</th>\n",
       "      <th>Post Message</th>\n",
       "      <th>Type</th>\n",
       "      <th>Countries</th>\n",
       "      <th>Languages</th>\n",
       "      <th>Posted</th>\n",
       "      <th>Audience Targeting</th>\n",
       "      <th>Lifetime Post Total Reach</th>\n",
       "      <th>Lifetime Post organic reach</th>\n",
       "      <th>...</th>\n",
       "      <th>Lifetime Matched Audience Targeting Consumptions by Type - other clicks</th>\n",
       "      <th>Lifetime Matched Audience Targeting Consumptions by Type - link clicks</th>\n",
       "      <th>Lifetime Matched Audience Targeting Consumptions by Type - photo view</th>\n",
       "      <th>Lifetime Matched Audience Targeting Consumptions by Type - video play</th>\n",
       "      <th>Lifetime Negative Feedback from Users by Type - hide_all_clicks</th>\n",
       "      <th>Lifetime Negative Feedback from Users by Type - hide_clicks</th>\n",
       "      <th>Lifetime Negative Feedback from Users by Type - report_spam_clicks</th>\n",
       "      <th>Lifetime Negative Feedback by Type - hide_all_clicks</th>\n",
       "      <th>Lifetime Negative Feedback by Type - hide_clicks</th>\n",
       "      <th>Lifetime Negative Feedback by Type - report_spam_clicks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lifetime: The number of people who had your Pa...</td>\n",
       "      <td>Lifetime: The number of people who had your Pa...</td>\n",
       "      <td>...</td>\n",
       "      <td>Lifetime: The number of clicks anywhere in the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lifetime: The number of times people have give...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lifetime: The number of people who have given ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>187446750783_10156099244995784</td>\n",
       "      <td>https://www.facebook.com/ibmwatson/posts/10156...</td>\n",
       "      <td>In 2018, IBM has made significant strides towa...</td>\n",
       "      <td>Link</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12/26/18 9:22</td>\n",
       "      <td></td>\n",
       "      <td>13560</td>\n",
       "      <td>13560</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>187446750783_10156082794855784</td>\n",
       "      <td>https://www.facebook.com/ibmwatson/posts/10156...</td>\n",
       "      <td>Available now on IBM Cloud and Cloud Private, ...</td>\n",
       "      <td>Photo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12/18/18 7:15</td>\n",
       "      <td></td>\n",
       "      <td>7837</td>\n",
       "      <td>7837</td>\n",
       "      <td>...</td>\n",
       "      <td>25</td>\n",
       "      <td>17.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>187446750783_10156079731075784</td>\n",
       "      <td>https://www.facebook.com/ibmwatson/posts/10156...</td>\n",
       "      <td>A year in review in AI: https://ibm.co/2QAlo29</td>\n",
       "      <td>Link</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12/16/18 14:54</td>\n",
       "      <td></td>\n",
       "      <td>10727</td>\n",
       "      <td>10727</td>\n",
       "      <td>...</td>\n",
       "      <td>39</td>\n",
       "      <td>129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>187446750783_10156067568775784</td>\n",
       "      <td>https://www.facebook.com/ibmwatson/posts/10156...</td>\n",
       "      <td>IBM Watson</td>\n",
       "      <td>SharedVideo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12/10/18 14:06</td>\n",
       "      <td></td>\n",
       "      <td>7937</td>\n",
       "      <td>7937</td>\n",
       "      <td>...</td>\n",
       "      <td>191</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Post ID  \\\n",
       "0                             NaN   \n",
       "1  187446750783_10156099244995784   \n",
       "2  187446750783_10156082794855784   \n",
       "3  187446750783_10156079731075784   \n",
       "4  187446750783_10156067568775784   \n",
       "\n",
       "                                           Permalink  \\\n",
       "0                                                NaN   \n",
       "1  https://www.facebook.com/ibmwatson/posts/10156...   \n",
       "2  https://www.facebook.com/ibmwatson/posts/10156...   \n",
       "3  https://www.facebook.com/ibmwatson/posts/10156...   \n",
       "4  https://www.facebook.com/ibmwatson/posts/10156...   \n",
       "\n",
       "                                        Post Message         Type  Countries  \\\n",
       "0                                                NaN          NaN        NaN   \n",
       "1  In 2018, IBM has made significant strides towa...         Link        NaN   \n",
       "2  Available now on IBM Cloud and Cloud Private, ...        Photo        NaN   \n",
       "3     A year in review in AI: https://ibm.co/2QAlo29         Link        NaN   \n",
       "4                                         IBM Watson  SharedVideo        NaN   \n",
       "\n",
       "   Languages          Posted Audience Targeting  \\\n",
       "0        NaN             NaN                NaN   \n",
       "1        NaN   12/26/18 9:22                      \n",
       "2        NaN   12/18/18 7:15                      \n",
       "3        NaN  12/16/18 14:54                      \n",
       "4        NaN  12/10/18 14:06                      \n",
       "\n",
       "                           Lifetime Post Total Reach  \\\n",
       "0  Lifetime: The number of people who had your Pa...   \n",
       "1                                              13560   \n",
       "2                                               7837   \n",
       "3                                              10727   \n",
       "4                                               7937   \n",
       "\n",
       "                         Lifetime Post organic reach  ...  \\\n",
       "0  Lifetime: The number of people who had your Pa...  ...   \n",
       "1                                              13560  ...   \n",
       "2                                               7837  ...   \n",
       "3                                              10727  ...   \n",
       "4                                               7937  ...   \n",
       "\n",
       "  Lifetime Matched Audience Targeting Consumptions by Type - other clicks  \\\n",
       "0  Lifetime: The number of clicks anywhere in the...                        \n",
       "1                                                 53                        \n",
       "2                                                 25                        \n",
       "3                                                 39                        \n",
       "4                                                191                        \n",
       "\n",
       "  Lifetime Matched Audience Targeting Consumptions by Type - link clicks  \\\n",
       "0                                                NaN                       \n",
       "1                                               84.0                       \n",
       "2                                               17.0                       \n",
       "3                                              129.0                       \n",
       "4                                               80.0                       \n",
       "\n",
       "  Lifetime Matched Audience Targeting Consumptions by Type - photo view  \\\n",
       "0                                                NaN                      \n",
       "1                                                NaN                      \n",
       "2                                               47.0                      \n",
       "3                                                NaN                      \n",
       "4                                                NaN                      \n",
       "\n",
       "  Lifetime Matched Audience Targeting Consumptions by Type - video play  \\\n",
       "0                                                NaN                      \n",
       "1                                                NaN                      \n",
       "2                                                NaN                      \n",
       "3                                                NaN                      \n",
       "4                                                NaN                      \n",
       "\n",
       "  Lifetime Negative Feedback from Users by Type - hide_all_clicks  \\\n",
       "0  Lifetime: The number of times people have give...                \n",
       "1                                                  3                \n",
       "2                                                  4                \n",
       "3                                                  3                \n",
       "4                                                  1                \n",
       "\n",
       "  Lifetime Negative Feedback from Users by Type - hide_clicks  \\\n",
       "0                                                NaN            \n",
       "1                                                4.0            \n",
       "2                                                NaN            \n",
       "3                                                4.0            \n",
       "4                                                4.0            \n",
       "\n",
       "  Lifetime Negative Feedback from Users by Type - report_spam_clicks  \\\n",
       "0                                                NaN                   \n",
       "1                                                NaN                   \n",
       "2                                                NaN                   \n",
       "3                                                NaN                   \n",
       "4                                                NaN                   \n",
       "\n",
       "  Lifetime Negative Feedback by Type - hide_all_clicks  \\\n",
       "0  Lifetime: The number of people who have given ...     \n",
       "1                                                  3     \n",
       "2                                                  4     \n",
       "3                                                  3     \n",
       "4                                                  1     \n",
       "\n",
       "  Lifetime Negative Feedback by Type - hide_clicks  \\\n",
       "0                                              NaN   \n",
       "1                                              4.0   \n",
       "2                                              NaN   \n",
       "3                                              4.0   \n",
       "4                                              4.0   \n",
       "\n",
       "  Lifetime Negative Feedback by Type - report_spam_clicks  \n",
       "0                                                NaN       \n",
       "1                                                NaN       \n",
       "2                                                NaN       \n",
       "3                                                NaN       \n",
       "4                                                NaN       \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import types\n",
    "import pandas as pd\n",
    "df_data_1 = pd.read_csv('./data/facebook_data.csv')\n",
    "df_data_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables:\n",
    " - The name of the DataFrame\n",
    " - Credentials for the source file\n",
    " - A file name for the enriched DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure this uses the variable above. The number will vary in the inserted code.\n",
    "try:\n",
    "    df = df_data_1\n",
    "except NameError as e:\n",
    "    print('Error: Setup is incorrect or incomplete.\\n')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_1 = {\n",
    "    'IAM_SERVICE_ID': 'iam-ServiceId-abff04b7-b6e9-4b78-8ea5-76efe48912de',\n",
    "    'IBM_API_KEY_ID': 'gYRYR_gfr-GnwdSKMXUYQHjn4u6p9cGNIfsMYcJA_K9z',\n",
    "    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'IBM_AUTH_ENDPOINT': 'https://iam.cloud.ibm.com/oidc/token',\n",
    "    'BUCKET': 'ivyhacks-donotdelete-pr-myhkmp8kqkr9uk',\n",
    "    'FILE': 'example_facebook_data.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credentials = credentials_1\n",
    "except NameError as e:\n",
    "    print('Error: Setup is incorrect or incomplete.\\n')\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data\n",
    "We'll prepare the data by cleansing it and extracting the URLs. \n",
    "Many of the posts contain both text and a URL. \n",
    "First we have to separate URLs from the text so that they can be analyzed separately. \n",
    "After that we have to get thumbnails for the photos and links, and convert any shortened URLs to full URLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleansing with Python\n",
    "\n",
    "Renaming columns, removing noticeable noise in the data, pulling out URLs and appending to a new column to run through NLU.\n",
    "\n",
    "To cleanse the data, we'll rename a column and add a column with the URLs that were embedded in the facebook post.   \n",
    "    `Post Message` column to `Text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Post Message': 'Text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows that have no value for the text.\n",
    "df.dropna(subset=['Text'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `str.partition` function to remove strings that contain \"http\" and \"www\" from the `Text` column and save them in new DataFrames, then we add all web addresses to a new `Link` column in the original DataFrame. This process captures all web addresses: https, http, and www."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_http = df[\"Text\"].str.partition(\"http\")\n",
    "df_www = df[\"Text\"].str.partition(\"www\")\n",
    "\n",
    "# Combine delimiters with actual links\n",
    "df_http[\"Link\"] = df_http[1].map(str) + df_http[2]\n",
    "df_www[\"Link1\"] = df_www[1].map(str) + df_www[2]\n",
    "\n",
    "# Include only Link columns\n",
    "df_http.drop(df_http.columns[0:3], axis=1, inplace = True)\n",
    "df_www.drop(df_www.columns[0:3], axis=1, inplace = True)\n",
    "\n",
    "# Merge http and www DataFrames\n",
    "dfmerge = pd.concat([df_http, df_www], axis=1)\n",
    "\n",
    "# The following steps will allow you to merge data columns from the left to the right\n",
    "dfmerge = dfmerge.apply(lambda x: x.str.strip()).replace('', np.nan)\n",
    "\n",
    "# Use fillna to fill any blanks with the Link1 column\n",
    "dfmerge[\"Link\"].fillna(dfmerge[\"Link1\"], inplace = True)\n",
    "\n",
    "# Delete Link1 (www column)\n",
    "dfmerge.drop(\"Link1\", axis=1, inplace = True)\n",
    "\n",
    "# Combine Link data frame\n",
    "df = pd.concat([dfmerge,df], axis = 1)\n",
    "\n",
    "# Make sure text column is a string\n",
    "df[\"Text\"] = df[\"Text\"].astype(\"str\")\n",
    "\n",
    "# Strip links from Text column\n",
    "df['Text'] = df['Text'].apply(lambda x: x.split('http')[0])\n",
    "df['Text'] = df['Text'].apply(lambda x: x.split('www')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting thumbnails and extended links\n",
    "\n",
    "A standard Facebook export does not provide the thumbnail that usually summarizes the link or photo associated with each post. Using the Beautiful Soup library to go into the HTML of the post and extract the thumbnail text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping url https://ibm.co/2zZaWrE: HTTPConnectionPool(host='softwaredownloads-prod.mrs-prod-7d4bdc08e7ddc90fa89b373d95c240eb-0000.us-south.containers.appdomain.cloud', port=80): Max retries exceeded with url: /mrs/https:/www.ibm.com/account/reg/us-en/signup?formid=urx-20732&cm_mmc=OSocial_Facebook-_-Watson%20Core_Watson%20Core%20-%20Conversation-_-WW_WW-_-Virtual%20Summit%20Recorded%20Session%20Sign%20Up%20Nov%2022&cm_mmca2=10004432&cm_mmca1=000027BD (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000204A0F64D48>: Failed to establish a new connection: [WinError 10060] Se produjo un error durante el intento de conexión ya que la parte conectada no respondió adecuadamente tras un periodo de tiempo, o bien se produjo un error en la conexión establecida ya que el host conectado no ha podido responder'))\n"
     ]
    }
   ],
   "source": [
    "# Changing links from objects to strings\n",
    "for link in df.Link:\n",
    "    df.Link.to_string()\n",
    "\n",
    "piclinks = []\n",
    "description = []\n",
    "for url in df[\"Link\"]:\n",
    "    if pd.isnull(url):\n",
    "        piclinks.append(\"\")\n",
    "        description.append(\"\")\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        # Skiping certificate check with verify=False.\n",
    "        page3 = requests.get(url, verify=False)\n",
    "        if page3.status_code != requests.codes.ok:\n",
    "            piclinks.append(\"\")\n",
    "            description.append(\"\")\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        print(\"Skipping url %s: %s\" % (url, e))\n",
    "        piclinks.append(\"\")\n",
    "        description.append(\"\")\n",
    "        continue\n",
    "        \n",
    "    soup3 = bs(page3.text,\"lxml\")\n",
    "    \n",
    "    pic = soup3.find('meta', property =\"og:image\")\n",
    "    if pic:\n",
    "        piclinks.append(pic[\"content\"])\n",
    "    else: \n",
    "        piclinks.append(\"\")\n",
    "    \n",
    "    content = None\n",
    "    desc = soup3.find(attrs={'name':'Description'})\n",
    "    if desc:\n",
    "        content = desc['content']\n",
    "    if not content or content == 'null':\n",
    "        # Try again with lowercase description\n",
    "        desc = soup3.find(attrs={'name':'description'})\n",
    "        if desc:\n",
    "            content = desc['content']\n",
    "    if not content or content == 'null':\n",
    "        description.append(\"\")\n",
    "    else:\n",
    "        description.append(content)\n",
    "            \n",
    "# Saving thumbnail descriptions to df in a column titled 'Thumbnails'\n",
    "df[\"Thumbnails\"] = description\n",
    "# Saving image links to df in a column titled 'Image'\n",
    "df[\"Image\"] = piclinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting shortened links to full links via requests module to pull extended links. This is only necessary if the Facebook page uses different links than the articles themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortlink = df[\"Link\"]\n",
    "extendedlink = []\n",
    "\n",
    "for link in shortlink:\n",
    "    if isinstance(link, float):  # Float is not a URL, probably NaN.\n",
    "        extendedlink.append('')\n",
    "    else:\n",
    "        try:\n",
    "            extended_link = requests.Session().head(link, allow_redirects=True).url\n",
    "            extendedlink.append(extended_link)\n",
    "        except Exception as e:\n",
    "            print(\"Skipping link %s: %s\" % (link, e))\n",
    "            extendedlink.append('')\n",
    "\n",
    "df[\"Extended Links\"] = extendedlink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLU for the Post Text\n",
    "\n",
    "The following script is an example of how to use Natural Language Understanding to iterate through each post and extract enrichment features for future analysis.\n",
    "\n",
    "For this example, we are looking at the `Text` column in our DataFrame, which contains the text of each post. NLU can also iterate through a column of URLs, or other freeform text. There's a list within a list for the Keywords and Entities features to allow gathering multiple entities and keywords from each piece of text.\n",
    "\n",
    "Each extracted feature is appended to the DataFrame in a new column that's defined at the end of the script. If you want to run this same script for the other columns, set the loop iterable to the column name, if you are using URLs, change the `text=response` parameter to `url=response`, and update the new column names as necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of features to get enrichment values for entities, keywords, emotion and sentiment\n",
    "features = Features(entities=EntitiesOptions(), keywords=KeywordsOptions(), emotion=EmotionOptions(), sentiment=SentimentOptions())\n",
    "\n",
    "overallSentimentScore = []\n",
    "overallSentimentType = []\n",
    "highestEmotion = []\n",
    "highestEmotionScore = []\n",
    "kywords = []\n",
    "entities = []\n",
    "\n",
    "# Go through every response and enrich the text using NLU.\n",
    "for text in df['Text']:\n",
    "  if not text:\n",
    "    # print(\"Text is empty\")\n",
    "    overallSentimentScore.append('0')\n",
    "    overallSentimentType.append('0')\n",
    "    highestEmotion.append(\"\")\n",
    "    highestEmotionScore.append(\"\")\n",
    "    kywords.append(\"\")\n",
    "    entities.append(\"\")\n",
    "    continue\n",
    "  else:\n",
    "    # We are assuming English to avoid errors when the language cannot be detected.\n",
    "    enriched_json = nlu.analyze(text=text, features=features, language='en').get_result()\n",
    "\n",
    "    # Get the SENTIMENT score and type\n",
    "    if 'sentiment' in enriched_json:\n",
    "        if('score' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentScore.append(enriched_json[\"sentiment\"][\"document\"][\"score\"])\n",
    "        else:\n",
    "            overallSentimentScore.append('0')\n",
    "\n",
    "        if('label' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentType.append(enriched_json[\"sentiment\"][\"document\"][\"label\"])\n",
    "        else:\n",
    "            overallSentimentType.append('0')\n",
    "    else:\n",
    "        overallSentimentScore.append('0')\n",
    "        overallSentimentType.append('0')\n",
    "\n",
    "    # Read the EMOTIONS into a dict and get the key (emotion) with maximum value\n",
    "    if 'emotion' in enriched_json:\n",
    "        me = max(enriched_json[\"emotion\"][\"document\"][\"emotion\"].items(), key=operator.itemgetter(1))[0]\n",
    "        highestEmotion.append(me)\n",
    "        highestEmotionScore.append(enriched_json[\"emotion\"][\"document\"][\"emotion\"][me])\n",
    "    else:\n",
    "        highestEmotion.append(\"\")\n",
    "        highestEmotionScore.append(\"\")\n",
    "\n",
    "    # Iterate and get KEYWORDS with a confidence of over 70%\n",
    "    if 'keywords' in enriched_json:\n",
    "        tmpkw = []\n",
    "        for kw in enriched_json['keywords']:\n",
    "            if(float(kw[\"relevance\"]) >= 0.7):\n",
    "                tmpkw.append(kw[\"text\"])\n",
    "        # Convert multiple keywords in a list to a string and append the string\n",
    "        kywords.append(', '.join(tmpkw))\n",
    "    else:\n",
    "        kywords.append(\"\")\n",
    "            \n",
    "    # Iterate and get Entities with a confidence of over 30%\n",
    "    if 'entities' in enriched_json:\n",
    "        tmpent = []\n",
    "        for ent in enriched_json['entities']: \n",
    "            if(float(ent[\"relevance\"]) >= 0.3):\n",
    "                tmpent.append(ent[\"type\"])\n",
    " \n",
    "        # Convert multiple entities in a list to a string and append the string\n",
    "        entities.append(', '.join(tmpent))\n",
    "    else:\n",
    "        entities.append(\"\")\n",
    "    \n",
    "# Create columns from the list and append to the DataFrame\n",
    "if highestEmotion:\n",
    "    df['TextHighestEmotion'] = highestEmotion\n",
    "if highestEmotionScore:\n",
    "    df['TextHighestEmotionScore'] = highestEmotionScore\n",
    "\n",
    "if overallSentimentType:\n",
    "    df['TextOverallSentimentType'] = overallSentimentType\n",
    "if overallSentimentScore:\n",
    "    df['TextOverallSentimentScore'] = overallSentimentScore\n",
    "\n",
    "df['TextKeywords'] = kywords\n",
    "df['TextEntities'] = entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we extract all of the Keywords and Entities from each Post, we have columns with multiple Keywords and Entities separated by commas. For our Analysis in Part II, we also wanted the top Keyword and Entity for each Post. Because of this, we added two new columns to capture the `MaxTextKeyword` and `MaxTextEntity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose first of Keywords and Entities\n",
    "df[\"MaxTextKeywords\"] = df[\"TextKeywords\"].apply(lambda x: x.split(',')[0])\n",
    "df[\"MaxTextEntity\"] = df[\"TextEntities\"].apply(lambda x: x.split(',')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLU for Thumbnail Text\n",
    "\n",
    "We will repeat the same process for Thumbnails and Article Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of features to get enrichment values for entities, keywords, emotion and sentiment\n",
    "features = Features(entities=EntitiesOptions(), keywords=KeywordsOptions(), emotion=EmotionOptions(), sentiment=SentimentOptions())\n",
    "\n",
    "overallSentimentScore = []\n",
    "overallSentimentType = []\n",
    "highestEmotion = []\n",
    "highestEmotionScore = []\n",
    "kywords = []\n",
    "entities = []\n",
    "\n",
    "# Go through every response and enrich the text using NLU.\n",
    "for text in df['Thumbnails']:\n",
    "    if not text:\n",
    "        overallSentimentScore.append(' ')\n",
    "        overallSentimentType.append(' ')\n",
    "        highestEmotion.append(' ')\n",
    "        highestEmotionScore.append(' ')\n",
    "        kywords.append(' ')\n",
    "        entities.append(' ')\n",
    "        continue\n",
    "\n",
    "    enriched_json = nlu.analyze(text=text, features=features, language='en').get_result()\n",
    "\n",
    "    # Get the SENTIMENT score and type\n",
    "    if 'sentiment' in enriched_json:\n",
    "        if('score' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentScore.append(enriched_json[\"sentiment\"][\"document\"][\"score\"])\n",
    "        else:\n",
    "            overallSentimentScore.append(\"\")\n",
    "\n",
    "        if('label' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentType.append(enriched_json[\"sentiment\"][\"document\"][\"label\"])\n",
    "        else:\n",
    "            overallSentimentType.append(\"\")\n",
    "\n",
    "    # Read the EMOTIONS into a dict and get the key (emotion) with maximum value\n",
    "    if 'emotion' in enriched_json:\n",
    "        me = max(enriched_json[\"emotion\"][\"document\"][\"emotion\"].items(), key=operator.itemgetter(1))[0]\n",
    "        highestEmotion.append(me)\n",
    "        highestEmotionScore.append(enriched_json[\"emotion\"][\"document\"][\"emotion\"][me])\n",
    "\n",
    "    else:\n",
    "        highestEmotion.append(\"\")\n",
    "        highestEmotionScore.append(\"\")\n",
    "\n",
    "    # Iterate and get KEYWORDS with a confidence of over 70%\n",
    "    if 'keywords' in enriched_json:\n",
    "        tmpkw = []\n",
    "        for kw in enriched_json['keywords']:\n",
    "            if(float(kw[\"relevance\"]) >= 0.7):\n",
    "                tmpkw.append(kw[\"text\"])\n",
    "        # Convert multiple keywords in a list to a string and append the string\n",
    "        kywords.append(', '.join(tmpkw))\n",
    "     \n",
    "    # Iterate and get Entities with a confidence of over 30%\n",
    "    if 'entities' in enriched_json:\n",
    "        tmpent = []\n",
    "        for ent in enriched_json['entities']:              \n",
    "            if(float(ent[\"relevance\"]) >= 0.3):\n",
    "                tmpent.append(ent[\"type\"])\n",
    "        # Convert multiple entities in a list to a string and append the string\n",
    "        entities.append(', '.join(tmpent))\n",
    "    else:\n",
    "        entities.append(\"\")     \n",
    "  \n",
    "# Create columns from the list and append to the DataFrame\n",
    "if highestEmotion:\n",
    "    df['ThumbnailHighestEmotion'] = highestEmotion\n",
    "if highestEmotionScore:\n",
    "    df['ThumbnailHighestEmotionScore'] = highestEmotionScore\n",
    "\n",
    "if overallSentimentType:\n",
    "    df['ThumbnailOverallSentimentType'] = overallSentimentType\n",
    "if overallSentimentScore:\n",
    "    df['ThumbnailOverallSentimentScore'] = overallSentimentScore\n",
    "\n",
    "df['ThumbnailKeywords'] = kywords\n",
    "df['ThumbnailEntities'] = entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Add two new columns to capture the `MaxThumbnailKeyword` and `MaxThumbnailEntity`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'Max' to first one from keywords and entities lists\n",
    "df[\"MaxThumbnailKeywords\"] = df[\"ThumbnailKeywords\"].apply(lambda x: x.split(',')[0])\n",
    "df[\"MaxThumbnailEntity\"] = df[\"ThumbnailEntities\"].apply(lambda x: x.split(',')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLU for Article Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of features to get enrichment values for entities, keywords, emotion and sentiment\n",
    "features = Features(entities=EntitiesOptions(), keywords=KeywordsOptions(), emotion=EmotionOptions(), sentiment=SentimentOptions())\n",
    "\n",
    "overallSentimentScore = []\n",
    "overallSentimentType = []\n",
    "highestEmotion = []\n",
    "highestEmotionScore = []\n",
    "kywords = []\n",
    "entities = []\n",
    "article_text = []\n",
    "        \n",
    "# Go through every response and enrich the article using NLU\n",
    "for url in df['Extended Links']:\n",
    "    if not url:\n",
    "        overallSentimentScore.append(' ')\n",
    "        overallSentimentType.append(' ')\n",
    "        highestEmotion.append(' ')\n",
    "        highestEmotionScore.append(' ')\n",
    "        kywords.append(' ')\n",
    "        entities.append(' ')\n",
    "        article_text.append(' ')\n",
    "        continue\n",
    "\n",
    "    # Run links through NLU to get entities, keywords, emotion and sentiment.\n",
    "    # Use return_analyzed_text to extract text for Tone Analyzer to use.\n",
    "    \n",
    "    try:\n",
    "        enriched_json = nlu.analyze(url=url,\n",
    "                                features=features,\n",
    "                                language='en',\n",
    "                                return_analyzed_text=True).get_result()\n",
    "        article_text.append(enriched_json[\"analyzed_text\"])\n",
    "    except Exception as e:\n",
    "        print(\"Skipping url %s: %s\" % (url, e))\n",
    "        overallSentimentScore.append(' ')\n",
    "        overallSentimentType.append(' ')\n",
    "        highestEmotion.append(' ')\n",
    "        highestEmotionScore.append(' ')\n",
    "        kywords.append(' ')\n",
    "        entities.append(' ')\n",
    "        article_text.append(' ')\n",
    "        continue\n",
    "      \n",
    "    \n",
    "    \n",
    "\n",
    "    # Get the SENTIMENT score and type\n",
    "    if 'sentiment' in enriched_json:\n",
    "        if('score' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentScore.append(enriched_json[\"sentiment\"][\"document\"][\"score\"])\n",
    "        else:\n",
    "            overallSentimentScore.append('None')\n",
    "\n",
    "        if('label' in enriched_json['sentiment'][\"document\"]):\n",
    "            overallSentimentType.append(enriched_json[\"sentiment\"][\"document\"][\"label\"])\n",
    "        else:\n",
    "            overallSentimentType.append('')\n",
    "\n",
    "    # Read the EMOTIONS into a dict and get the key (emotion) with maximum value\n",
    "    if 'emotion' in enriched_json:\n",
    "        me = max(enriched_json[\"emotion\"][\"document\"][\"emotion\"].items(), key=operator.itemgetter(1))[0]\n",
    "        highestEmotion.append(me)\n",
    "        highestEmotionScore.append(enriched_json[\"emotion\"][\"document\"][\"emotion\"][me])\n",
    "\n",
    "    else:\n",
    "        highestEmotion.append('')\n",
    "        highestEmotionScore.append('')\n",
    "\n",
    "    # Iterate and get KEYWORDS with a confidence of over 70%\n",
    "    if 'keywords' in enriched_json:\n",
    "        tmpkw = []\n",
    "        for kw in enriched_json['keywords']:\n",
    "            if(float(kw[\"relevance\"]) >= 0.7):\n",
    "                tmpkw.append(kw[\"text\"])\n",
    "        # Convert multiple keywords in a list to a string and append the string\n",
    "        kywords.append(', '.join(tmpkw))\n",
    "    else: \n",
    "        kywords.append(\"\")\n",
    "            \n",
    "    # Iterate and get Entities with a confidence of over 30%\n",
    "    if 'entities' in enriched_json:\n",
    "        tmpent = []\n",
    "        for ent in enriched_json['entities']:               \n",
    "            if(float(ent[\"relevance\"]) >= 0.3):\n",
    "                tmpent.append(ent[\"type\"])\n",
    "        # Convert multiple entities in a list to a string and append the string\n",
    "        entities.append(', '.join(tmpent))\n",
    "    else:\n",
    "        entities.append(\"\")\n",
    "    \n",
    "# Create columns from the list and append to the DataFrame\n",
    "if highestEmotion:\n",
    "    df['LinkHighestEmotion'] = highestEmotion\n",
    "if highestEmotionScore:\n",
    "    df['LinkHighestEmotionScore'] = highestEmotionScore\n",
    "\n",
    "if overallSentimentType:\n",
    "    df['LinkOverallSentimentType'] = overallSentimentType\n",
    "if overallSentimentScore:\n",
    "    df['LinkOverallSentimentScore'] = overallSentimentScore\n",
    "\n",
    "df['LinkKeywords'] = kywords\n",
    "df['LinkEntities'] = entities\n",
    "df['Article Text'] = article_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add two new columns to capture the `MaxLinkKeyword` and `MaxLinkEntity`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'Max' to first one from keywords and entities lists\n",
    "df[\"MaxLinkKeywords\"] = df[\"LinkKeywords\"].apply(lambda x: x.split(',')[0])\n",
    "df[\"MaxLinkEntity\"] = df[\"LinkEntities\"].apply(lambda x: x.split(',')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enrichment is now COMPLETE  \n",
    "Save a copy of the enriched DataFrame as a file in Cloud Object Storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n",
    "    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n",
    "    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url=credentials['ENDPOINT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the enriched file name from the original filename.\n",
    "localfilename = 'enriched_' + credentials['FILE']\n",
    "\n",
    "# Write a CSV file from the enriched pandas DataFrame.\n",
    "df.to_csv(localfilename, index=False)\n",
    "\n",
    "# Use the above put_file method with credentials to put the file in Object Storage.\n",
    "cos.upload_file(localfilename, Bucket=credentials['BUCKET'],Key=localfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to use the enriched local file, you can read it back in.\n",
    "# This might be handy if you already enriched and just want to re-run\n",
    "# from this cell and below. Uncomment the following line.\n",
    "\n",
    "# df = pd.read_csv(localfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part II - Data Preparation\n",
    "\n",
    "Prepare Multiple DataFrames for Visualizations\n",
    "Before we can create the separate tables for each Watson feature we need to organize and reformat the data. First, we need to determine which data points are tied to metrics. Second, we need to make sure make sure each metric is numeric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the lifetime metrics in a list\n",
    "metrics = [metric for metric in df.columns.values.tolist() if 'Lifetime' in metric]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Consolidated Sentiment and Emotion DataFrame\n",
    "we'll create a DataFrame for the sentiment and emotion of the post text and a DataFrame for the sentiment and emotion of the article text. Then you'll combine them into one DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Sentiment and Emotion DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with only Post sentiment and emotion values\n",
    "post_tones = [\"Text\",\"TextHighestEmotion\", \"TextHighestEmotionScore\", \"TextOverallSentimentType\", \"TextOverallSentimentScore\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "post_tones.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with metrics and sentiment and emotion\n",
    "df_post_tones = df[post_tones]\n",
    "\n",
    "# Determine which tone values are suppose to be numeric and ensure they are numeric. \n",
    "post_numeric_values = [\"TextHighestEmotionScore\", \"TextOverallSentimentScore\"]\n",
    "for i in post_numeric_values:\n",
    "    df_post_tones[i] = pd.to_numeric(df_post_tones[i], errors='coerce')\n",
    "\n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_post_tones[i] = pd.to_numeric(df_post_tones[i], errors='coerce')\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_post_tones[\"Type\"] = \"Post\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Article Sentiment and Emotion DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with only Article sentiment and emotion values\n",
    "article_tones = [\"Text\", \"LinkHighestEmotion\", \"LinkHighestEmotionScore\", \"LinkOverallSentimentType\", \"LinkOverallSentimentScore\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "article_tones.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with metrics and sentiment and emotion\n",
    "df_article_tones = df[article_tones]\n",
    "\n",
    "# Determine which values are suppose to be numeric and ensure they are numeric. \n",
    "art_numeric_values = [\"LinkHighestEmotionScore\", \"LinkOverallSentimentScore\"]\n",
    "for i in art_numeric_values:\n",
    "    df_article_tones[i] = pd.to_numeric(df_article_tones[i], errors='coerce')\n",
    "    \n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_article_tones[i] = pd.to_numeric(df_article_tones[i], errors='coerce')\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_article_tones[\"Type\"] = \"Article\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Post and Article DataFrames to Make DataFrame with Sentiment and Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make the Column Headers the same\n",
    "df_post_tones.rename(columns={\"TextHighestEmotion\":\"Emotion\",\n",
    "                              \"TextHighestEmotionScore\":\"Emotion Score\",\n",
    "                              \"TextOverallSentimentType\": \"Sentiment\",\n",
    "                              \"TextOverallSentimentScore\": \"Sentiment Score\"\n",
    "                             },\n",
    "                     inplace=True)\n",
    "\n",
    "df_article_tones.rename(columns={\"LinkHighestEmotion\":\"Emotion\",\n",
    "                                 \"LinkHighestEmotionScore\":\"Emotion Score\",\n",
    "                                 \"LinkOverallSentimentType\": \"Sentiment\",\n",
    "                                 \"LinkOverallSentimentScore\": \"Sentiment Score\"\n",
    "                                },\n",
    "                        inplace=True)\n",
    "\n",
    "# Combine into one data frame\n",
    "df_tones = pd.concat([df_post_tones, df_article_tones])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the positive, neutral, and negative sentiments. The others are empty or unusable.\n",
    "df_tones = df_tones[df_tones.Sentiment.isin(['positive', 'neutral', 'negative'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Consolidated Keyword DataFrame\n",
    "DataFrames for the keywords of the article text, the thumbnail text, and the post text. Then you'll combine them into one DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Article Keyword DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with only Article Keywords\n",
    "article_keywords = [\"Text\", \"MaxLinkKeywords\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "article_keywords.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with keywords and metrics\n",
    "df_article_keywords = df[article_keywords]\n",
    "\n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_article_keywords[i] = pd.to_numeric(df_article_keywords[i], errors='coerce')\n",
    "\n",
    "# Drop NA Values in Keywords Column\n",
    "df_article_keywords['MaxLinkKeywords'].replace(' ', np.nan, inplace=True)\n",
    "df_article_keywords.dropna(subset=['MaxLinkKeywords'], inplace=True)\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_article_keywords[\"Type\"] = \"Article\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thumbnail Keyword DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with only Thumbnail Keywords\n",
    "thumbnail_keywords = [\"Text\", \"MaxThumbnailKeywords\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "thumbnail_keywords.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with keywords and metrics\n",
    "df_thumbnail_keywords = df[thumbnail_keywords]\n",
    "\n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_thumbnail_keywords[i] = pd.to_numeric(df_thumbnail_keywords[i], errors='coerce')\n",
    "    \n",
    "# Drop NA Values in Keywords Column\n",
    "df_thumbnail_keywords['MaxThumbnailKeywords'].replace(' ', np.nan, inplace=True)\n",
    "df_thumbnail_keywords.dropna(subset=['MaxThumbnailKeywords'], inplace=True)\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_thumbnail_keywords[\"Type\"] = \"Thumbnails\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Keyword DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with only Thumbnail Keywords\n",
    "post_keywords = [\"Text\", \"MaxTextKeywords\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "post_keywords.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with keywords and metrics\n",
    "df_post_keywords = df[post_keywords]\n",
    "\n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_post_keywords[i] = pd.to_numeric(df_post_keywords[i], errors='coerce')\n",
    "    \n",
    "# Drop NA Values in Keywords Column\n",
    "df_post_keywords['MaxTextKeywords'].replace(' ', np.nan, inplace=True)\n",
    "df_post_keywords.dropna(subset=['MaxTextKeywords'], inplace=True)\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_post_keywords[\"Type\"] = \"Posts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Post, Thumbnail, and Article DataFrames to Make One Keywords DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make the column headers the same\n",
    "df_post_keywords.rename(columns={\"MaxTextKeywords\": \"Keywords\"}, inplace=True)\n",
    "df_thumbnail_keywords.rename(columns={\"MaxThumbnailKeywords\":\"Keywords\"}, inplace=True)\n",
    "df_article_keywords.rename(columns={\"MaxLinkKeywords\":\"Keywords\"}, inplace=True)\n",
    "\n",
    "# Combine into one data frame\n",
    "df_keywords = pd.concat([df_post_keywords, df_thumbnail_keywords, df_article_keywords])\n",
    "\n",
    "# Discard posts with lower total reach to make charting easier\n",
    "df_keywords = df_keywords[df_keywords[\"Lifetime Post Total Reach\"] > 20000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Consolidated Entity DataFrame\n",
    "You'll create DataFrames for the entities of the article text, the thumbnail text, and the post text. Then you'll combine them into one DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Article Entity DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with only Article Keywords\n",
    "article_entities = [\"Text\", \"MaxLinkEntity\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "article_entities.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with keywords and metrics\n",
    "df_article_entities = df[article_entities]\n",
    "    \n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_article_entities[i] = pd.to_numeric(df_article_entities[i], errors='coerce')\n",
    "    \n",
    "# Drop NA Values in Keywords Column\n",
    "df_article_entities['MaxLinkEntity'] = df[\"MaxLinkEntity\"].replace(r'\\s+', np.nan, regex=True)\n",
    "df_article_entities.dropna(subset=['MaxLinkEntity'], inplace=True)\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_article_entities[\"Type\"] = \"Article\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thumbnail Entity DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with only Thumbnail Keywords\n",
    "thumbnail_entities = [\"Text\", \"MaxThumbnailEntity\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "thumbnail_entities.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with keywords and metrics\n",
    "df_thumbnail_entities = df[thumbnail_entities]\n",
    "\n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_thumbnail_entities[i] = pd.to_numeric(df_thumbnail_entities[i], errors='coerce')\n",
    "    \n",
    "# Drop NA Values in Keywords Column\n",
    "df_thumbnail_entities['MaxThumbnailEntity'] = df_thumbnail_entities['MaxThumbnailEntity'].replace(r'\\s+', np.nan, regex=True)\n",
    "df_thumbnail_entities.dropna(subset=['MaxThumbnailEntity'], inplace=True)\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_thumbnail_entities[\"Type\"] = \"Thumbnails\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post Entity DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list with only Thumbnail Keywords\n",
    "post_entities = [\"Text\", \"MaxTextEntity\"]\n",
    "\n",
    "# Append DataFrame with these metrics\n",
    "post_entities.extend(metrics)\n",
    "\n",
    "# Create a new DataFrame with keywords and metrics\n",
    "df_post_entities = df[post_entities]\n",
    "\n",
    "# Make all metrics numeric\n",
    "for i in metrics:\n",
    "    df_post_entities[i] = pd.to_numeric(df_post_entities[i], errors='coerce')\n",
    "    \n",
    "# Drop NA Values in Keywords Column\n",
    "df_post_entities['MaxTextEntity'] = df_post_entities['MaxTextEntity'].replace(r'\\s+', np.nan, regex=True)\n",
    "df_post_entities.dropna(subset=['MaxTextEntity'], inplace=True)\n",
    "\n",
    "# Add in a column to distinguish what portion the enrichment was happening \n",
    "df_post_entities[\"Type\"] = \"Posts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Post, Thumbnail, and Article DataFrames to Make One Entity DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make the column headers the same\n",
    "df_post_entities.rename(columns={\"MaxTextEntity\": \"Entities\"}, inplace=True)\n",
    "\n",
    "df_thumbnail_entities.rename(columns={\"MaxThumbnailEntity\":\"Entities\"}, inplace=True)\n",
    "\n",
    "df_article_entities.rename(columns={\"MaxLinkEntity\":\"Entities\"}, inplace=True)\n",
    "\n",
    "# Combine into one data frame\n",
    "df_entities = pd.concat([df_post_entities, df_thumbnail_entities, df_article_entities])\n",
    "\n",
    "df_entities[\"Entities\"] = df_entities[\"Entities\"].replace('', np.nan)\n",
    "df_entities.dropna(subset=[\"Entities\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Consolidated Image DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine Metrics with Type Hierarchy, Class and Color to Make One Image DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visual_recognition:\n",
    "    # Create a list with only Visual Recognition columns\n",
    "    pic_keywords = ['Image Type', 'Image Subtype', 'Image Subtype2', 'Image Class', 'Image Color']\n",
    "\n",
    "    # Append DataFrame with these metrics\n",
    "    pic_keywords.extend(metrics)\n",
    "\n",
    "    # Create a new DataFrame with keywords and metrics\n",
    "    df_pic_keywords = df[pic_keywords]\n",
    "\n",
    "    # Make all metrics numeric\n",
    "    for i in metrics:\n",
    "        df_pic_keywords[i] = pd.to_numeric(df_pic_keywords[i], errors='coerce')\n",
    "\n",
    "    # Discard posts with lower total reach to make charting easier\n",
    "    df_pic_keywords = df_pic_keywords[df_pic_keywords[\"Lifetime Post Total Reach\"] > 15000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visual_recognition:\n",
    "    images = df_pic_keywords[df_pic_keywords['Image Type'] != ' ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3rd Step\n",
    "\n",
    "Setup\n",
    "Assign Variables\n",
    "Assign new DataFrames to variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = df_entities\n",
    "tones = df_tones\n",
    "keywords = df_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Data\n",
    "Run PixieDust Visualization Library with Display() API\n",
    "PixieDust lets you visualize your data in just a few clicks using the display() API. You can find more info at https://pixiedust.github.io/pixiedust/displayapi.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can use a pie chart to identify how lifetime engagement was broken up by sentiment. \n",
    "\n",
    "Click on the `Options` button to change the chart.  Here are some things to try:\n",
    "* Add *Type* to make the breakdown show *Post* or *Article*.\n",
    "* Show *Emotion* intead of *Sentiment* (or both).\n",
    "* Try a different metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "chartsize": "70",
      "charttype": "stacked",
      "clusterby": "Type",
      "handlerId": "pieChart",
      "keyFields": "Emotion",
      "legend": "true",
      "mpld3": "false",
      "orientation": "horizontal",
      "rendererId": "matplotlib",
      "rowCount": "100",
      "sortby": "Values DESC",
      "title": "Lifetime Engaged Users by Emotion",
      "valueFields": "Lifetime Engaged Users",
      "ylabel": "true"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(tones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at the same statistics as a bar chart.\n",
    "\n",
    "It is the same line of code. Use the `Edit Metadata` button to see how PixieDust knows to show us a bar chart. If you don't have a button use the menu and select `View > Cell Toolbar > Edit Metadata`.\n",
    "\n",
    "A bar chart is better at showing more information. We added `Cluster By: Type` so we already see numbers for posts and articles. Notice what the chart tells you. Most of our articles and posts are `positive`. But what sentiment really engages more users?  Click on `Options` and try this:\n",
    "\n",
    "* Change the aggregation to `AVG`.\n",
    "\n",
    "What sentiment leads to higher average engagement?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "chartsize": "70",
      "charttype": "stacked",
      "clusterby": "Type",
      "handlerId": "barChart",
      "keyFields": "Sentiment",
      "legend": "true",
      "orientation": "horizontal",
      "rendererId": "matplotlib",
      "rowCount": "100",
      "sortby": "Values DESC",
      "title": "Lifetime Engaged Users by Sentiment",
      "valueFields": "Lifetime Engaged Users"
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(tones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at the entities that were detected by Natural Language Understanding.\n",
    "\n",
    "The following bar chart shows the entities that were detected. This time we are stacking negative feedback and \"likes\" to get a picture of the kind of feedback the entities were getting. We chose a horizontal, stacked bar chart with descending values for a little variety.\n",
    "\n",
    "* Try a different renderer and see what you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "chartsize": "70",
      "charttype": "stacked",
      "handlerId": "barChart",
      "keyFields": "Entities",
      "orientation": "horizontal",
      "rendererId": "matplotlib",
      "rowCount": "100",
      "sortby": "Values DESC",
      "title": "Entities in Posts and Articles",
      "valueFields": "Lifetime Post Stories by action type - like,Lifetime Negative Feedback from Users"
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we look at the keywords detected by Natural Language Understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "aggregation": "SUM",
      "chartsize": "85",
      "charttype": "stacked",
      "clusterby": "Type",
      "handlerId": "barChart",
      "keyFields": "Keywords",
      "legend": "true",
      "mpld3": "false",
      "orientation": "horizontal",
      "rendererId": "matplotlib",
      "rowCount": "100",
      "sortby": "Values DESC",
      "timeseries": "false",
      "title": "Keyword Total Reach",
      "valueFields": "Lifetime Post Total Reach"
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(keywords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
